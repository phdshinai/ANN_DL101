{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746ed65d-9331-40af-883c-b0302584665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6f6c84-f4dc-45a3-9fa8-f1c829ad7855",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data #####\n",
    "data = \"ë‚˜ë¼ì˜ ë§ì´ ì¤‘êµ­ê³¼ ë‹¬ë¼ ë¬¸ìì™€ ì„œë¡œ í†µí•˜ì§€ ì•„ë‹ˆí•˜ê¸°ì— ì´ëŸ° ê¹Œë‹­ìœ¼ë¡œ ì–´ë¦¬ì„ì€ ë°±ì„±ì´ ì´ë¥´ê³ ì í•  ë°”ê°€ ìˆì–´ë„ ë§ˆì¹¨ë‚´ ì œ ëœ»ì„ ëŠ¥íˆ í´ì§€ ëª»í•  ì‚¬ëŒì´ ë§ìœ¼ë‹ˆë¼ ë‚´ê°€ ì´ë¥¼ ìœ„í•´ ê°€ì—¾ì´ ì—¬ê²¨ ìƒˆë¡œ ìŠ¤ë¬¼ì—¬ëŸ ê¸€ìë¥¼ ë§Œë“œë…¸ë‹ˆ ì‚¬ëŒë§ˆë‹¤ í•˜ì—¬ ì‰¬ì´ ìµí˜€ ë‚ ë¡œ ì”€ì— í¸ì•ˆì¼€ í•˜ê³ ì í•  ë”°ë¦„ì´ë‹ˆë¼\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c452481c-bded-43e2-a689-6037bb9ccc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë°ì´í„°ë¥¼ preprocessing í•´ì£¼ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤\n",
    "#RNNê³¼ ë™ì¼í•œ ë°©ë²•ì…ë‹ˆë‹¤\n",
    "def data_preprocessing(data):\n",
    "    data = re.sub('[^ê°€-í£]', ' ', data)\n",
    "    tokens = data.split()\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    Word_to_ix = {Word: i for i, Word in enumerate(vocab)}\n",
    "    ix_to_Word = {i: Word for i, Word in enumerate(vocab)}\n",
    "\n",
    "    return tokens, vocab_size, Word_to_ix, ix_to_Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8388c374-ec27-4ad7-b79d-8d666cbcf65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™œì„±í™” í•¨ìˆ˜\n",
    "def sigmoid(input):\n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def sigmoid_derivative(input):\n",
    "    return input * (1 - input)\n",
    "    \n",
    "def tanh(input, derivative=False):\n",
    "    return np.tanh(input)\n",
    "\n",
    "def tanh_derivative(input):\n",
    "    return 1 - input ** 2\n",
    "\n",
    "def softmax(input):\n",
    "    return np.exp(input) / np.sum(np.exp(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56753766-f611-489b-8882-0b79b57b67ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Forget Gate\n",
    "        self.Wf = np.random.randn(hidden_size, input_size)*0.1\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Input Gate\n",
    "        self.Wi = np.random.randn(hidden_size, input_size)*0.1\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Candidate Gate\n",
    "        self.Wc = np.random.randn(hidden_size, input_size)*0.1\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Output Gate\n",
    "        self.Wo = np.random.randn(hidden_size, input_size)*0.1\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Final Gate\n",
    "        self.Wy = np.random.randn(output_size, hidden_size)\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "    # ë„¤íŠ¸ì›Œí¬ ë©”ëª¨ë¦¬ ë¦¬ì…‹\n",
    "    def reset(self):\n",
    "        self.X = {}\n",
    "\n",
    "        self.HS = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.CS = {-1: np.zeros((self.hidden_size, 1))}\n",
    "\n",
    "        self.C = {}\n",
    "        self.O = {}\n",
    "        self.F = {}\n",
    "        self.I = {}\n",
    "        self.outputs = {}\n",
    "\n",
    "    # Forward ìˆœì „íŒŒ\n",
    "    def forward(self, inputs):\n",
    "        # self.reset()\n",
    "        x = {}\n",
    "        outputs = []\n",
    "        for t in range(len(inputs)):\n",
    "            x[t] = np.zeros((vocab_size , 1))\n",
    "            x[t][inputs[t]] = 1  # ê°ê°ì˜ Wordì— ëŒ€í•œ one hot coding\n",
    "            self.X[t] = np.concatenate((self.HS[t - 1], x[t]))\n",
    "\n",
    "            self.F[t] = sigmoid(np.dot(self.Wf, self.X[t]) + self.bf)\n",
    "            self.I[t] = sigmoid(np.dot(self.Wi, self.X[t]) + self.bi)\n",
    "            self.C[t] = tanh(np.dot(self.Wc, self.X[t]) + self.bc)\n",
    "            self.O[t] = sigmoid(np.dot(self.Wo, self.X[t]) + self.bo)\n",
    "\n",
    "            self.CS[t] = self.F[t] * self.CS[t - 1] + self.I[t] * self.C[t]\n",
    "            self.HS[t] = self.O[t] * tanh(self.CS[t])\n",
    "\n",
    "            outputs += [np.dot(self.Wy, self.HS[t]) + self.by]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # ì—­ì „íŒŒ\n",
    "    def backward(self, errors, inputs):\n",
    "        dLdWf, dLdbf = 0, 0\n",
    "        dLdWi, dLdbi = 0, 0\n",
    "        dLdWc, dLdbc = 0, 0\n",
    "        dLdWo, dLdbo = 0, 0\n",
    "        dLdWy, dLdby = 0, 0\n",
    "\n",
    "        dh_next, dc_next = np.zeros_like(self.HS[0]), np.zeros_like(self.CS[0])\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            error = errors[t]\n",
    "\n",
    "            # Final Gate Weights and Biases Errors\n",
    "            dLdWy += np.dot(error, self.HS[t].T)         #ğœ•ğ¿/ğœ•ğ‘Šğ‘¦\n",
    "            dLdby += error                               #ğœ•ğ¿/ğœ•bğ‘¦ = (ğœ•ğ¿/ğœ•z_t)(ğœ•z_t/ğœ•bğ‘¦) = error x 1 (Zt = WyHSt + by)\n",
    "            \n",
    "            # Hidden State Error\n",
    "            dLdHS = np.dot(self.Wy.T, error) + dh_next    #ğœ•ğ¿/ğœ•ğ»ğ‘†\n",
    "\n",
    "            # Output Gate Weights and Biases Errors\n",
    "            dLdo = tanh(self.CS[t]) * dLdHS * sigmoid_derivative(self.O[t])\n",
    "            dLdWo += np.dot(dLdo, inputs[t].T)\n",
    "            dLdbo += dLdo\n",
    "\n",
    "            # Cell State Error\n",
    "            dLdCS = tanh_derivative(tanh(self.CS[t])) * self.O[t] * dLdHS + dc_next\n",
    "\n",
    "            # Forget Gate Weights and Biases Errors\n",
    "            dLdf = dLdCS * self.CS[t - 1] * sigmoid_derivative(self.F[t])\n",
    "            dLdWf += np.dot(dLdf, inputs[t].T)\n",
    "            dLdbf += dLdf\n",
    "\n",
    "            # Input Gate Weights and Biases Errors\n",
    "            dLdi = dLdCS * self.C[t] * sigmoid_derivative(self.I[t])\n",
    "            dLdWi += np.dot(dLdi, inputs[t].T)\n",
    "            dLdbi += dLdi\n",
    "\n",
    "            # Candidate Gate Weights and Biases Errors\n",
    "            dLdc = dLdCS * self.I[t] * tanh_derivative(self.C[t])\n",
    "            dLdWc += np.dot(dLdc, inputs[t].T)\n",
    "            dLdbc += dLdc\n",
    "\n",
    "            # Concatenated Input Error (Sum of Error at Each Gate!)\n",
    "            d_z = np.dot(self.Wf.T, dLdf) + np.dot(self.Wi.T, dLdi) + np.dot(self.Wc.T, dLdc) + np.dot(self.Wo.T, dLdo)\n",
    "\n",
    "            # Error of Hidden State and Cell State at Next Time Step\n",
    "            dh_next = d_z[:self.hidden_size, :]\n",
    "            dc_next = self.F[t] * dLdCS\n",
    "            \n",
    "        for d_ in (dLdWf, dLdbf, dLdWi, dLdbi, dLdWc, dLdbc, dLdWo, dLdbo, dLdWy, dLdby):\n",
    "            np.clip(d_, -1, 1, out=d_)\n",
    "\n",
    "        self.Wf += dLdWf * self.learning_rate * (-1)\n",
    "        self.bf += dLdbf * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wi += dLdWi * self.learning_rate * (-1)\n",
    "        self.bi += dLdbi * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wc += dLdWc * self.learning_rate * (-1)\n",
    "        self.bc += dLdbc * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wo += dLdWo * self.learning_rate * (-1)\n",
    "        self.bo += dLdbo * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wy += dLdWy * self.learning_rate * (-1)\n",
    "        self.by += dLdby * self.learning_rate * (-1)\n",
    "\n",
    "    # Train\n",
    "    def train(self, inputs, labels):\n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            self.reset()\n",
    "            input_idx = [Word_to_ix[input] for input in inputs]\n",
    "            predictions = self.forward(input_idx)\n",
    "\n",
    "            errors = []\n",
    "            for t in range(len(predictions)):\n",
    "                errors += [softmax(predictions[t])]\n",
    "                errors[-1][Word_to_ix[labels[t]]] -= 1\n",
    "\n",
    "            self.backward(errors, self.X)\n",
    "\n",
    "    def test(self, inputs, labels):\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward([Word_to_ix[input] for input in inputs])\n",
    "\n",
    "        gt = ''\n",
    "        output = 'ë‚˜ë¼ì˜ '\n",
    "        for q in range(len(labels)):\n",
    "            prediction = ix_to_Word[np.argmax(softmax(probabilities[q].reshape(-1)))]\n",
    "            gt += inputs[q] + ' '\n",
    "            output += prediction + ' '\n",
    "            \n",
    "            if prediction == labels[q]:\n",
    "                accuracy += 1\n",
    "\n",
    "        print('ì‹¤ì œê°’: ', gt)\n",
    "        print('ì˜ˆì¸¡ê°’: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6edb6f-28b6-466a-9977-a8fadfd2fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:02<00:00, 371.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‹¤ì œê°’:  ë‚˜ë¼ì˜ ë§ì´ ì¤‘êµ­ê³¼ ë‹¬ë¼ ë¬¸ìì™€ ì„œë¡œ í†µí•˜ì§€ ì•„ë‹ˆí•˜ê¸°ì— ì´ëŸ° ê¹Œë‹­ìœ¼ë¡œ ì–´ë¦¬ì„ì€ ë°±ì„±ì´ ì´ë¥´ê³ ì í•  ë°”ê°€ ìˆì–´ë„ ë§ˆì¹¨ë‚´ ì œ ëœ»ì„ ëŠ¥íˆ í´ì§€ ëª»í•  ì‚¬ëŒì´ ë§ìœ¼ë‹ˆë¼ ë‚´ê°€ ì´ë¥¼ ìœ„í•´ ê°€ì—¾ì´ ì—¬ê²¨ ìƒˆë¡œ ìŠ¤ë¬¼ì—¬ëŸ ê¸€ìë¥¼ ë§Œë“œë…¸ë‹ˆ ì‚¬ëŒë§ˆë‹¤ í•˜ì—¬ ì‰¬ì´ ìµí˜€ ë‚ ë¡œ ì”€ì— í¸ì•ˆì¼€ í•˜ê³ ì í•  \n",
      "ì˜ˆì¸¡ê°’:  ë‚˜ë¼ì˜ ë§ì´ ì¤‘êµ­ê³¼ ë‹¬ë¼ ë¬¸ìì™€ ì„œë¡œ í†µí•˜ì§€ ì•„ë‹ˆí•˜ê¸°ì— ì´ëŸ° ê¹Œë‹­ìœ¼ë¡œ ì–´ë¦¬ì„ì€ ë°±ì„±ì´ ì´ë¥´ê³ ì í•  ë°”ê°€ ìˆì–´ë„ ë§ˆì¹¨ë‚´ ì œ ëœ»ì„ ëŠ¥íˆ í´ì§€ ëª»í•  ì‚¬ëŒì´ ë§ìœ¼ë‹ˆë¼ ë‚´ê°€ ì´ë¥¼ ìœ„í•´ ê°€ì—¾ì´ ì—¬ê²¨ ìƒˆë¡œ ìŠ¤ë¬¼ì—¬ëŸ ê¸€ìë¥¼ ë§Œë“œë…¸ë‹ˆ ì‚¬ëŒë§ˆë‹¤ í•˜ì—¬ ì‰¬ì´ ìµí˜€ ë‚ ë¡œ ì”€ì— í¸ì•ˆì¼€ í•˜ê³ ì í•  ë”°ë¦„ì´ë‹ˆë¼ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 25\n",
    "\n",
    "# data preparation\n",
    "tokens, vocab_size, Word_to_ix, ix_to_Word = data_preprocessing(data)\n",
    "train_X, train_y = tokens[:-1], tokens[1:]\n",
    "\n",
    "lstm = LSTM(input_size=vocab_size + hidden_size, hidden_size=hidden_size, output_size=vocab_size, num_epochs=1000,\n",
    "            learning_rate=0.05)\n",
    "\n",
    "##### Training #####\n",
    "lstm.train(train_X, train_y)\n",
    "\n",
    "lstm.test(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c599bf-92f5-4c3c-9042-06c7843229fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_env",
   "language": "python",
   "name": "fusion_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
