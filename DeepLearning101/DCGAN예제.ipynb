{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb88129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7422cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is Z, going into a convolution (c x 100 x 1)\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # State size. c x 512 x 4 x 4\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # State size. c x 256 x 8 x 8\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # State size. c x 128 x 16 x 16\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # State size. 64 x 32 x 32\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # State size. 3 x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is 3 x 64 x 64\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size. 64 x 32 x 32\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size. 128 x 16 x 16\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size. 256 x 8 x 8\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size. 512 x 4 x 4\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63df014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generated_images(images, num_images=64):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Generated Images\")\n",
    "    images = vutils.make_grid(images[:num_images], padding=2, normalize=True)\n",
    "    images = np.transpose(images.cpu(), (1, 2, 0))\n",
    "    plt.imshow(images)\n",
    "    plt.show()\n",
    "\n",
    "def save_generated_images(images, num_images, epoch, idx):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Generated Images\")\n",
    "    images = vutils.make_grid(images[:num_images], padding=2, normalize=True)\n",
    "    images = np.transpose(images.cpu(), (1, 2, 0))\n",
    "    # plt.imshow(images)\n",
    "    fname = './output/image_'+str(epoch)+'_'+str(idx)+'.jpg'\n",
    "    plt.imsave(fname, images.numpy())\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c596a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator and the discriminator\n",
    "netG = Generator()\n",
    "netD = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae56a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the weights_init function to randomly initialize all weights\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d4114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='./data/celeba', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f51271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG.to(device)\n",
    "netD.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "num_epochs = 10  # For demonstration purposes; increase for better results\n",
    "\n",
    "fixed_noise = torch.randn(64, 100, 1, 1, device=device)         # intermediate visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # Update Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        netD.zero_grad()\n",
    "        real_data = data[0].to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "        real_label = torch.full((batch_size,), 1, dtype=torch.float, device=device)\n",
    "        fake_label = torch.full((batch_size,), 0, dtype=torch.float, device=device)\n",
    "\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_data).view(-1)\n",
    "        errD_real = criterion(output, real_label)\n",
    "        errD_real.backward()\n",
    "\n",
    "        # Generate fake image batch with G\n",
    "        # noise = torch.randn(batch_size, 100, device=device)\n",
    "        noise = torch.randn(batch_size, 100, 1,1, device=device)\n",
    "        fake_data = netG(noise)\n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "\n",
    "        # Calculate D's loss on the fake batch\n",
    "        errD_fake = criterion(output, fake_label)\n",
    "        errD_fake.backward()\n",
    "\n",
    "        # Add the gradients from the real and fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Update Generator: maximize log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        output = netD(fake_data).view(-1)\n",
    "        errG = criterion(output, real_label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item()))\n",
    "            # fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
    "            fake_images = netG(fixed_noise)\n",
    "            save_generated_images(fake_images, 64, epoch=epoch, idx=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bbddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "fake_images = netG(fixed_noise)\n",
    "show_generated_images(fake_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf638c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_env",
   "language": "python",
   "name": "fusion_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
